{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа № 1 \n",
    "## Выполнение разведочного анализа больших данных с использованием фреймворка Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1\n",
    "\n",
    "В данной части работы рассмотрены:\n",
    "* загрузка данных из HDFS;\n",
    "* базовые преобразования данных;\n",
    "* загрузка преобразованных данных в таблицу `Apache Airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключим необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    regexp_extract_all,\n",
    "    col,\n",
    "    lit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем объект конфигурации для `Apache Spark`, указав необходимые параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_configuration() -> SparkConf:\n",
    "    \"\"\"\n",
    "    Создает и конфигурирует экземпляр SparkConf для приложения Spark.\n",
    "\n",
    "    Returns:\n",
    "        SparkConf: Настроенный экземпляр SparkConf.\n",
    "    \"\"\"\n",
    "    # Получаем имя пользователя\n",
    "    user_name = os.getenv(\"USER\")\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"lab 1 Test\")\n",
    "    conf.setMaster(\"yarn\")\n",
    "    conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "    conf.set(\"spark.executor.memory\", \"12g\")\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.executor.instances\", \"2\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.cores\", \"2\")\n",
    "    conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0\")\n",
    "    conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.warehouse\", f\"hdfs:///user/{user_name}/warehouse\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём сам объект конфигурации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = create_spark_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём и выводим на экран сессию `Apache Spark`. В процессе создания сессии происходит подключение к кластеру `Apache Hadoop`, что может занять некоторое время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/user7/.ivy2/cache\n",
      "The jars for the packages stored in: /home/user7/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-22650cad-606e-4662-ad44-b3fb9d7e432b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0 in central\n",
      ":: resolution report :: resolve 658ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-22650cad-606e-4662-ad44-b3fb9d7e432b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/27ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/12 04:20:23 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/12/12 04:20:43 WARN Client: Same path resource file:///home/user7/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.6.0.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://node32.cluster:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab 1 Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f394493b750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для исследования был выбран датасет `\"NYC Yellow Taxi Trip Data\"`, расположенный на платформе `Kaggle` по адресу https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data.\n",
    "\n",
    "Датасет включает в себя информацию о более чем двенадцати миллионах поездок на такси в городе Нью-Йорк и его окрестностях за 2015-ый год.\n",
    "\n",
    "Указываем путь в `HDFS` для файла с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs:///user/user7/skobelin_dir/taxi_1.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполняем датафрейм данными из файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим фрагмент датафрейма на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|   pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       2| 2015-01-15 19:05:39|  2015-01-15 19:23:42|              1|         1.59|   -73.993896484375|40.750110626220703|         1|                 N|-73.974784851074219|40.750617980957031|           1|         12|    1|    0.5|      3.25|           0|                  0.3|       17.05|\n",
      "|       1| 2015-01-10 20:33:38|  2015-01-10 20:53:28|              1|         3.30| -74.00164794921875|  40.7242431640625|         1|                 N|-73.994415283203125|40.759109497070313|           1|       14.5|  0.5|    0.5|         2|           0|                  0.3|        17.8|\n",
      "|       1| 2015-01-10 20:33:38|  2015-01-10 20:43:41|              1|         1.80|-73.963340759277344|40.802787780761719|         1|                 N|-73.951820373535156|40.824413299560547|           2|        9.5|  0.5|    0.5|         0|           0|                  0.3|        10.8|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:35:31|              1|          .50|-74.009086608886719|40.713817596435547|         1|                 N|-74.004325866699219|40.719985961914063|           2|        3.5|  0.5|    0.5|         0|           0|                  0.3|         4.8|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:52:58|              1|         3.00|-73.971176147460938|40.762428283691406|         1|                 N|-74.004180908203125|40.742652893066406|           2|         15|  0.5|    0.5|         0|           0|                  0.3|        16.3|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:53:52|              1|         9.00|-73.874374389648438|  40.7740478515625|         1|                 N|-73.986976623535156|40.758193969726563|           1|         27|  0.5|    0.5|       6.7|        5.33|                  0.3|       40.33|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:58:31|              1|         2.20|  -73.9832763671875|40.726009368896484|         1|                 N|-73.992469787597656|  40.7496337890625|           2|         14|  0.5|    0.5|         0|           0|                  0.3|        15.3|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:42:20|              3|          .80|-74.002662658691406|40.734142303466797|         1|                 N|-73.995010375976563|40.726325988769531|           1|          7|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 21:11:35|              3|        18.20|-73.783042907714844|40.644355773925781|         2|                 N|-73.987594604492187|40.759357452392578|           2|         52|    0|    0.5|         0|        5.33|                  0.3|       58.13|\n",
      "|       1| 2015-01-10 20:33:40|  2015-01-10 20:40:44|              2|          .90|-73.985588073730469|40.767948150634766|         1|                 N|-73.985916137695313|40.759365081787109|           1|        6.5|  0.5|    0.5|      1.55|           0|                  0.3|        9.35|\n",
      "|       1| 2015-01-10 20:33:40|  2015-01-10 20:41:39|              1|          .90|-73.988616943359375|40.723102569580078|         1|                 N|    -74.00439453125|40.728584289550781|           1|          7|  0.5|    0.5|      1.66|           0|                  0.3|        9.96|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 20:43:26|              1|         1.10|-73.993782043457031|40.751419067382812|         1|                 N|  -73.9674072265625|40.757217407226563|           1|        7.5|  0.5|    0.5|         1|           0|                  0.3|         9.8|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 20:35:23|              1|          .30| -74.00836181640625|40.704376220703125|         1|                 N|-74.009773254394531|40.707725524902344|           2|          3|  0.5|    0.5|         0|           0|                  0.3|         4.3|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 21:03:04|              1|         3.10|-73.973945617675781|40.760448455810547|         1|                 N|-73.997344970703125|40.735210418701172|           1|         19|  0.5|    0.5|         3|           0|                  0.3|        23.3|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 20:39:23|              1|         1.10|-74.006721496582031|40.731777191162109|         1|                 N|-73.995216369628906|40.739894866943359|           2|          6|  0.5|    0.5|         0|           0|                  0.3|         7.3|\n",
      "|       2| 2015-01-15 19:05:39|  2015-01-15 19:32:00|              1|         2.38|-73.976425170898437|40.739810943603516|         1|                 N|-73.983978271484375|40.757888793945313|           1|       16.5|    1|    0.5|      4.38|           0|                  0.3|       22.68|\n",
      "|       2| 2015-01-15 19:05:40|  2015-01-15 19:21:00|              5|         2.83|-73.968704223632812|40.754245758056641|         1|                 N|-73.955123901367188|40.786857604980469|           2|       12.5|    1|    0.5|         0|           0|                  0.3|        14.3|\n",
      "|       2| 2015-01-15 19:05:40|  2015-01-15 19:28:18|              5|         8.33|-73.863059997558594|40.769580841064453|         1|                 N|-73.952713012695312|40.785781860351563|           1|         26|    1|    0.5|      8.08|        5.33|                  0.3|       41.21|\n",
      "|       2| 2015-01-15 19:05:41|  2015-01-15 19:20:36|              1|         2.37|-73.945541381835938|40.779422760009766|         1|                 N|-73.980850219726563|40.786083221435547|           1|       11.5|    1|    0.5|         0|           0|                  0.3|        13.3|\n",
      "|       2| 2015-01-15 19:05:41|  2015-01-15 19:20:22|              2|         7.13|-73.874458312988281|40.774009704589844|         1|                 N|-73.952377319335938|40.718589782714844|           1|       21.5|    1|    0.5|       4.5|           0|                  0.3|        27.8|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "# VendorID - id компании, в которой работал таксист\n",
    "# RateCode - способ расчета суммы оплаты\n",
    "# StoreAndForward - показывает, хранилась ли информация о поездке в памяти машины перед отправкой на сервер или нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расшифруем названия колонок:\n",
    "\n",
    "| Название столбца  | Расшифровка |\n",
    "| ------------- | ------------- |\n",
    "| VendorID              | Id компании, в которой работал таксист  |\n",
    "| tpep_pickup_datetime  | Дата и время начала поездки  |\n",
    "| tpep_dropoff_datetime | Дата и вреям окончания поездки |\n",
    "| passenger_count       | Количество пассажиров |\n",
    "| trip_distance         | Расстояние (в милях) между начальной и конечной токами поездки |\n",
    "| pickup_longitude      | Долгота на которой был подобран пассажир |\n",
    "| pickup_latitude       | Широта на которой был подобран пассажир |\n",
    "| RateCodeID            | Способ рассчета суммы оплаты |\n",
    "| store_and_fwd_flag    | Показывает, хранилась ли информация о поездке в памяти машины перед отправкой на сервер или нет |\n",
    "| dropoff_longitude     | Долгота на которой был высажен пассажир |\n",
    "| dropoff_latitude      | Широта на которой был высажен пассажир |\n",
    "| payment_type          | Способ оплаты |\n",
    "| fare_amount           | Базовая стоимость поездки |\n",
    "| extra                 | Доплата |\n",
    "| mta_tax               | Налог |\n",
    "| tip_amount            | Чаевые |\n",
    "| tolls_amount          | Налог с длительных поездок |\n",
    "| improvement_surcharge | Дополнительный сбор средств на улучшение качества услуг |\n",
    "| total_amount          | Полная стоимость поездки |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем на экран метаданные датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12748986"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что все столбцы датасета содержат строковый тип данных, что не соответствует ожиданиям. Выполним преобразования типов данных некоторых столбцов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Преобразует столбцы DataFrame в указанные типы данных и\n",
    "    выполняет необходимые преобразования.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Исходный DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Преобразованный DataFrame.\n",
    "    \"\"\"\n",
    "    # Преобразуем столбцы в соответствующие типы данных\n",
    "    data = data.withColumn(\"VendorID\",\n",
    "                           col(\"VendorID\").cast(\"Integer\"))\n",
    "    data = data.withColumn(\"passenger_count\",\n",
    "                           col(\"passenger_count\").cast(\"Integer\"))\n",
    "    data = data.withColumn(\"RateCodeID\",\n",
    "                           col(\"RateCodeID\").cast(\"Integer\"))\n",
    "    data = data.withColumn(\"payment_type\",\n",
    "                           col(\"payment_type\").cast(\"Integer\"))\n",
    "    data = data.withColumn(\"trip_distance\",\n",
    "                           col(\"trip_distance\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"pickup_longitude\",\n",
    "                           col(\"pickup_longitude\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"pickup_latitude\",\n",
    "                           col(\"pickup_latitude\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"dropoff_longitude\",\n",
    "                           col(\"dropoff_longitude\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"dropoff_latitude\",\n",
    "                           col(\"dropoff_latitude\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"fare_amount\",\n",
    "                           col(\"fare_amount\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"extra\",\n",
    "                           col(\"extra\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"mta_tax\",\n",
    "                           col(\"mta_tax\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"tip_amount\",\n",
    "                           col(\"tip_amount\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"tolls_amount\",\n",
    "                           col(\"tolls_amount\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"improvement_surcharge\",\n",
    "                           col(\"improvement_surcharge\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"total_amount\",\n",
    "                           col(\"total_amount\").cast(\"Float\"))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------------+---------------+----------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pickup_longitude|pickup_latitude|RateCodeID|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------------+---------------+----------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       2| 2015-01-15 19:05:39|  2015-01-15 19:23:42|              1|         1.59|        -73.9939|       40.75011|         1|                 N|       -73.974785|       40.750618|           1|       12.0|  1.0|    0.5|      3.25|         0.0|                  0.3|       17.05|\n",
      "|       1| 2015-01-10 20:33:38|  2015-01-10 20:53:28|              1|          3.3|       -74.00165|      40.724243|         1|                 N|       -73.994415|        40.75911|           1|       14.5|  0.5|    0.5|       2.0|         0.0|                  0.3|        17.8|\n",
      "|       1| 2015-01-10 20:33:38|  2015-01-10 20:43:41|              1|          1.8|       -73.96334|      40.802788|         1|                 N|        -73.95182|       40.824413|           2|        9.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        10.8|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:35:31|              1|          0.5|       -74.00909|      40.713818|         1|                 N|       -74.004326|       40.719986|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:52:58|              1|          3.0|      -73.971176|       40.76243|         1|                 N|        -74.00418|       40.742653|           2|       15.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        16.3|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:53:52|              1|          9.0|      -73.874374|      40.774048|         1|                 N|        -73.98698|       40.758194|           1|       27.0|  0.5|    0.5|       6.7|        5.33|                  0.3|       40.33|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:58:31|              1|          2.2|       -73.98328|       40.72601|         1|                 N|        -73.99247|       40.749634|           2|       14.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        15.3|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 20:42:20|              3|          0.8|       -74.00266|      40.734142|         1|                 N|        -73.99501|       40.726326|           1|        7.0|  0.5|    0.5|      1.66|         0.0|                  0.3|        9.96|\n",
      "|       1| 2015-01-10 20:33:39|  2015-01-10 21:11:35|              3|         18.2|       -73.78304|      40.644356|         2|                 N|       -73.987595|       40.759357|           2|       52.0|  0.0|    0.5|       0.0|        5.33|                  0.3|       58.13|\n",
      "|       1| 2015-01-10 20:33:40|  2015-01-10 20:40:44|              2|          0.9|       -73.98559|       40.76795|         1|                 N|       -73.985916|       40.759365|           1|        6.5|  0.5|    0.5|      1.55|         0.0|                  0.3|        9.35|\n",
      "|       1| 2015-01-10 20:33:40|  2015-01-10 20:41:39|              1|          0.9|       -73.98862|      40.723103|         1|                 N|       -74.004395|       40.728584|           1|        7.0|  0.5|    0.5|      1.66|         0.0|                  0.3|        9.96|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 20:43:26|              1|          1.1|       -73.99378|       40.75142|         1|                 N|        -73.96741|       40.757217|           1|        7.5|  0.5|    0.5|       1.0|         0.0|                  0.3|         9.8|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 20:35:23|              1|          0.3|       -74.00836|      40.704376|         1|                 N|        -74.00977|       40.707726|           2|        3.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.3|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 21:03:04|              1|          3.1|      -73.973946|       40.76045|         1|                 N|       -73.997345|        40.73521|           1|       19.0|  0.5|    0.5|       3.0|         0.0|                  0.3|        23.3|\n",
      "|       1| 2015-01-10 20:33:41|  2015-01-10 20:39:23|              1|          1.1|       -74.00672|      40.731777|         1|                 N|        -73.99522|       40.739895|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|\n",
      "|       2| 2015-01-15 19:05:39|  2015-01-15 19:32:00|              1|         2.38|      -73.976425|       40.73981|         1|                 N|        -73.98398|        40.75789|           1|       16.5|  1.0|    0.5|      4.38|         0.0|                  0.3|       22.68|\n",
      "|       2| 2015-01-15 19:05:40|  2015-01-15 19:21:00|              5|         2.83|      -73.968704|      40.754246|         1|                 N|       -73.955124|       40.786858|           2|       12.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        14.3|\n",
      "|       2| 2015-01-15 19:05:40|  2015-01-15 19:28:18|              5|         8.33|       -73.86306|       40.76958|         1|                 N|        -73.95271|        40.78578|           1|       26.0|  1.0|    0.5|      8.08|        5.33|                  0.3|       41.21|\n",
      "|       2| 2015-01-15 19:05:41|  2015-01-15 19:20:36|              1|         2.37|       -73.94554|      40.779423|         1|                 N|        -73.98085|       40.786083|           1|       11.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        13.3|\n",
      "|       2| 2015-01-15 19:05:41|  2015-01-15 19:20:22|              2|         7.13|       -73.87446|       40.77401|         1|                 N|        -73.95238|        40.71859|           1|       21.5|  1.0|    0.5|       4.5|         0.0|                  0.3|        27.8|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------------+---------------+----------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- improvement_surcharge: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что теперь столбцы датафрейма содержат значения корректных типов.\n",
    "\n",
    "Полученный датафрейм сохраним для дальнейшего использования. Сохранение выполним в таблицу `Apache Iceberg`. \n",
    "\n",
    "`Apache Iceberg` — это поддерживающий высокую производительность табличный формат для больших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"skobelin_database\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим инструкцию SQL для добавления базы данных в каталог `Apache Spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_database_sql = f\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS spark_catalog.{database_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(create_database_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим созданную базу данных как текущую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, записываем преобразованный датафрейм в таблицу `sobd_lab1_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `skobelin_database`.`skobelin_lab1_table` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Сохранение DataFrame в виде таблицы\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskobelin_lab1_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musing\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda-spark-envs/pyspark-env/lib/python3.11/site-packages/pyspark/sql/readwriter.py:2078\u001b[0m, in \u001b[0;36mDataFrameWriterV2.create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;124;03m    Create a new table from the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \n\u001b[1;32m   2075\u001b[0m \u001b[38;5;124;03m    The new table's schema, partition layout, properties, and other configuration will be\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;124;03m    based on the configuration set on this writer.\u001b[39;00m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2078\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda-spark-envs/pyspark-env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda-spark-envs/pyspark-env/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `skobelin_database`.`skobelin_lab1_table` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects."
     ]
    }
   ],
   "source": [
    "# Сохранение DataFrame в виде таблицы\n",
    "df.writeTo(\"skobelin_lab1_table\").using(\"iceberg\").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После успешной записи можно посмотреть, какие таблицы входят в базу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skobelin_lab1_processed_table\n",
      "skobelin_lab1_table\n"
     ]
    }
   ],
   "source": [
    "for table in spark.catalog.listTables():\n",
    "    print(table.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После успешной записи таблицы останавливаем сессию `Apache Spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
